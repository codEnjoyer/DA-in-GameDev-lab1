# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Чашкин Никита Андреевич
- РИ210941

Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

## Цель работы
Интегрировать экономическую систему в проект Unity и обучить ML-Agent.

## Задание 1
### Изменить параметры файла .yaml-агента и определить какие параметры и как влияют на обучение модели.

Прежде чем менять параметры конфигурации нейронной сети, было бы неплохо обучить её на предоставленных данных, чтобы в будущем сравнить с изменёнными.

После загрузки проекта и установки mlagents packages я начал обучение нейронки.

![первое обучение](https://user-images.githubusercontent.com/87475288/205329131-d1dd8d4c-328c-4284-8882-19ea445bf229.jpg)

Очевидно, что процесс обучения пойдёт быстрее, если добавить больше префабов, на которых сетка учится. Так чего же мы ждём?

![первое обучение 12 копий](https://user-images.githubusercontent.com/87475288/205329318-e2433ea4-d234-459a-91d3-5846e051c578.jpg)

Спустя недолгое время обучение было завершено (а точнее прервано мной),

![закончил первое обучение](https://user-images.githubusercontent.com/87475288/205329653-d06fce2f-e9b9-4377-9a05-908a35ef0a9a.jpg)

и на основании полученных данных tensorboard любезно построил следующие графики:

![графики первого запуска 1](https://user-images.githubusercontent.com/87475288/205329757-94d90974-1937-4723-9ea2-ed4c095db92b.jpg)

![графики первого запуска 2](https://user-images.githubusercontent.com/87475288/205329765-6031eb38-2303-459a-8e3a-c8a913668cfe.jpg)

Нейросеть обучалась, следуя этой конфигурации:

![первый конфиг](https://user-images.githubusercontent.com/87475288/205329974-1ead9d64-977f-43f9-8b29-3f88dd3799f6.jpg)

И вот настал момент, когда можно залезть в .yaml файл и намешать там грязи! Однако, не желая всё сломать, а потом чинить несколько часов (хотя что тут вообще можно сломать), я добавил лишь небольшие не особо осознанные корректировки:

buffer_size => было 10_240 / стало 15_000
beta => было 1.0e-2 / стало 5.0e-1
checkpoint_interval => было 500_000 / стало 50_000
max_steps => было 750_000 / стало 100_000
time_horizon => было 64 / стало 128
summary_freq => было 5000 / стало 0

За что отвечает каждый из этих параметров? Ответ в задании №2.

![второй конфиг](https://user-images.githubusercontent.com/87475288/205330510-8cfe1225-3a0a-419e-8f3f-a60b900ff1a2.jpg)
 
После внесённых изменений я начал обучение вновь, но, как оказалось, всё-таки что-то сломал:

![на ноль делить нельзя](https://user-images.githubusercontent.com/87475288/205331381-7ee833ea-1ba7-4719-9090-a9d49f2050a9.jpg)

Изменение параметра summary_freq с 5000 до 0 не понравилось mlagent, в какой-то момент во время обучения он делил на этот параметр, а, как мы знаем, делить на ноль нельзя! Было принято решение вернуть значение этого параметра, как было, и не трогать больше, работает ведь.

Запустил обучение, получил результаты, смотрим:

![графики второго запуска 1](https://user-images.githubusercontent.com/87475288/205332009-5d95b1e8-4e7d-4906-a038-dc395581646d.jpg)

![графики второго запуска 2](https://user-images.githubusercontent.com/87475288/205332023-5aefdf2b-7c91-46b9-b71a-837bdcffdc72.jpg)

А почему так получилось? А это уже задание №2.


## Задание 2
### Описать результаты, выведенные в TensorBoard.

Итак, сперва укажем, для чего вообще нужны те параметры, которые были изменены в .yaml файле. О некоторых из них я уже писал в третьей лабораторной работе.

#### buffer_size :
    Количество опыта, которое нужно набрать для обновления политики модели.
    Было увеличено.
#### beta :
    Случайность действия. Повышает разнообразие и иследованность пространства обучения.
    Было увеличено.
#### checkpoint_interval :
    Количество опыта, собираемое между каждым чекпоинтом.
    Было уменьшено.
#### max_steps :
    Количество шагов, которые должны быть выполнены в среде для завершения обучения.
    Было уменьшено.
#### time_horizon :
    Количество циклов ML-агента, необходимых перед добавлением в буфер опыта.
    Было увеличено.
#### summary_freq :
    Количество опыта, необходимого перед созданием и отображением статистики.
    Осталось без изменений.

На графиках первого обучения видно, как Cumulative Reward (накопительное вознаграждение) стремительно растёт, ведь нейросеть обучается и получает с каждым разом всё большую награду. Episode Length (длительнось эпизода) и Policy Loss (политика реагирования на неудачи) оставались стабильными. Value Loss (потеря значимости обучения) стремительно падает, из чего так же следует, что нейросеть обучается удачно. К тому же можно отметить падение Epsilon (порог расхождений между старой и новой политиками) и повышение Extrinsic Reward (награда окружения).

На удивление, второе обучение показало себя лучше. Замечу, что длительность обучения была повышена, а количество шагов удвоено. За счёт увеличения опыта, необходимого для закрепления результата (buffer_size, time_horizon), нейросеть начала показывать стабильно высокий результат уже спустя 10_000 шагов. Относительно первых графиков параметр beta стал сильнее, но всё же некритично, снижаться, хотя энтропия выросла. Epsilon так же стал снижаться сильнее.

Могу сказать, что изменения конфигурации обучения нейросети положительно сказались на итоговом результате.

## Выводы
Благодаря этой работе я глубже изучил параметры обучения нейросетей, познакомился с очень полезным инструментом для отображения графиков - TensorBoard.
Я поражаюсь самому факту существования нейросетей, ведь в их основе заложена обыкновенная (хоть и далеко не простая) математика. С их помощью можно творить невероятные вещи, сложно представить, во что выльется их развитие в ближайшие 10 лет, но не сложно понять, что их влияние на нас будет огромным.

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
